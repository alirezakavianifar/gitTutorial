{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMqg/9qZrGzh+/sZr5R353",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/gitTutorial/blob/developer/Bypass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm9WTUPaHK4A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "# Initialize parameters\n",
        "np.random.seed(0)  # For reproducibility\n",
        "W1 = np.random.randn(3, 2)\n",
        "W2 = np.random.randn(1, 3)\n",
        "W_bypass = np.random.randn(1, 2)\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Inputs (batch of 4)\n",
        "P = np.array([\n",
        "    [1, 2],\n",
        "    [2, -3],\n",
        "    [0, 2],\n",
        "    [-1, -4]\n",
        "])\n",
        "\n",
        "# Labels\n",
        "Y = np.array([2.2, 7.5, 9.2, 8.1])\n",
        "\n",
        "# Forward pass\n",
        "def forward(P, W1, W2, W_bypass):\n",
        "    A1 = np.dot(P, W1.T)\n",
        "    H1 = relu(A1)\n",
        "    A2 = np.dot(H1, W2.T) + np.dot(P, W_bypass.T)\n",
        "    H2 = linear(A2)\n",
        "    return H1, H2\n",
        "\n",
        "# Compute MSE loss\n",
        "def compute_loss(H2, Y):\n",
        "    loss = np.mean((H2 - Y) ** 2)\n",
        "    return loss\n",
        "\n",
        "# Backward pass (Gradient computation)\n",
        "def backward(P, H1, H2, Y, W1, W2, W_bypass):\n",
        "    dH2 = 2 * (H2 - Y) / len(Y)  # Derivative of MSE with respect to H2\n",
        "    dW2 = np.dot(dH2.T, H1)  # Gradient for W2\n",
        "    dW_bypass = np.dot(dH2.T, P)  # Gradient for W_bypass\n",
        "\n",
        "    dH1 = np.dot(dH2, W2)  # Propagate error to H1\n",
        "    dA1 = dH1 * (H1 > 0)  # Derivative of ReLU\n",
        "\n",
        "    dW1 = np.dot(dA1.T, P)  # Gradient for W1\n",
        "\n",
        "    return dW1, dW2, dW_bypass\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    H1, H2 = forward(P, W1, W2, W_bypass)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(H2, Y)\n",
        "\n",
        "    # Backward pass\n",
        "    dW1, dW2, dW_bypass = backward(P, H1, H2, Y, W1, W2, W_bypass)\n",
        "\n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * dW1\n",
        "    W2 -= learning_rate * dW2\n",
        "    W_bypass -= learning_rate * dW_bypass\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "# Print final parameters\n",
        "print(\"Final parameters:\")\n",
        "print(\"W1:\", W1)\n",
        "print(\"W2:\", W2)\n",
        "print(\"W_bypass:\", W_bypass)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "# Derivative of activation functions\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Mean Squared Error loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Derivative of Mean Squared Error loss function\n",
        "def mse_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# Neural Network class with bypass\n",
        "class BypassNetwork:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size)\n",
        "        self.W2 = np.random.randn(1, hidden_size)\n",
        "        self.W_bypass = np.random.randn(1, input_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, p):\n",
        "        self.p = p\n",
        "        self.a1 = np.dot(self.W1, self.p)\n",
        "        self.h1 = relu(self.a1)\n",
        "        self.h2 = np.dot(self.W2, self.h1) + np.dot(self.W_bypass, self.p)\n",
        "        return linear(self.h2)\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        # Calculate gradients\n",
        "        loss_grad = mse_derivative(y_true, y_pred)\n",
        "        dW2 = loss_grad * self.h1\n",
        "        dW_bypass = loss_grad * self.p\n",
        "        dW1 = np.dot((loss_grad * self.W2 * relu_derivative(self.a1)).reshape(-1, 1), self.p.reshape(1, -1))\n",
        "\n",
        "        # Update weights\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.W_bypass -= self.learning_rate * dW_bypass\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "\n",
        "    def train(self, inputs, labels, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for p, y_true in zip(inputs, labels):\n",
        "                y_pred = self.forward(p)\n",
        "                loss = mse_loss(y_true, y_pred)\n",
        "                total_loss += loss\n",
        "                self.backward(y_true, y_pred)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {total_loss / len(inputs)}')\n",
        "\n",
        "# Training data\n",
        "inputs = [np.array([1, 2]), np.array([2, -3]), np.array([0, 2]), np.array([-1, -4])]\n",
        "labels = [2.2, 7.5, 9.2, 8.1]\n",
        "\n",
        "# Initialize and train the network\n",
        "network = BypassNetwork(input_size=2, hidden_size=3, learning_rate=0.01)\n",
        "network.train(inputs, labels, epochs=1000)\n",
        "\n",
        "# Print final weights\n",
        "print(\"W1:\", network.W1)\n",
        "print(\"W2:\", network.W2)\n",
        "print(\"W_bypass:\", network.W_bypass)"
      ],
      "metadata": {
        "id": "FKxqiSSkIeb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "# Derivative of activation functions\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Mean Squared Error loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Derivative of Mean Squared Error loss function\n",
        "def mse_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# Neural Network class for the top architecture\n",
        "class BypassNetworkTop:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size)\n",
        "        self.W2 = np.random.randn(1, hidden_size)\n",
        "        self.W_bypass = np.random.randn(1, input_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, p):\n",
        "        self.p = p\n",
        "        self.a1 = np.dot(self.W1, self.p)\n",
        "        self.h1 = relu(self.a1)\n",
        "        self.h2 = np.dot(self.W2, self.h1) + np.dot(self.W_bypass, self.p)\n",
        "        return linear(self.h2)\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        loss_grad = mse_derivative(y_true, y_pred)\n",
        "        dW2 = loss_grad * self.h1\n",
        "        dW_bypass = loss_grad * self.p\n",
        "        dW1 = np.dot((loss_grad * self.W2 * relu_derivative(self.a1)).reshape(-1, 1), self.p.reshape(1, -1))\n",
        "\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.W_bypass -= self.learning_rate * dW_bypass\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "\n",
        "    def train(self, inputs, labels, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for p, y_true in zip(inputs, labels):\n",
        "                y_pred = self.forward(p)\n",
        "                loss = mse_loss(y_true, y_pred)\n",
        "                total_loss += loss\n",
        "                self.backward(y_true, y_pred)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {total_loss / len(inputs)}')\n",
        "\n",
        "# Neural Network class for the bottom architecture\n",
        "class BypassNetworkBottom:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size)\n",
        "        self.W2 = np.random.randn(1, hidden_size)\n",
        "        self.W_bypass = np.random.randn(1, input_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, p):\n",
        "        self.p = p\n",
        "        self.a1 = np.dot(self.W1, self.p)\n",
        "        self.h1 = relu(self.a1)\n",
        "        self.h2 = np.dot(self.W2, self.h1) + np.dot(self.W_bypass, self.p)\n",
        "        return linear(self.h2)\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        loss_grad = mse_derivative(y_true, y_pred)\n",
        "        dW2 = loss_grad * self.h1\n",
        "        dW_bypass = loss_grad * self.p\n",
        "        dW1 = np.dot((loss_grad * self.W2 * relu_derivative(self.a1)).reshape(-1, 1), self.p.reshape(1, -1))\n",
        "\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.W_bypass -= self.learning_rate * dW_bypass\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "\n",
        "    def train(self, inputs, labels, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for p, y_true in zip(inputs, labels):\n",
        "                y_pred = self.forward(p)\n",
        "                loss = mse_loss(y_true, y_pred)\n",
        "                total_loss += loss\n",
        "                self.backward(y_true, y_pred)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {total_loss / len(inputs)}')\n",
        "\n",
        "# Training data\n",
        "inputs = [np.array([1, 2]), np.array([2, -3]), np.array([0, 2]), np.array([-1, -4])]\n",
        "labels = [2.2, 7.5, 9.2, 8.1]\n",
        "\n",
        "# Initialize and train the top architecture network\n",
        "print(\"Training Top Architecture Network\")\n",
        "network_top = BypassNetworkTop(input_size=2, hidden_size=3, learning_rate=0.01)\n",
        "network_top.train(inputs, labels, epochs=1000)\n",
        "\n",
        "# Print final weights of the top architecture\n",
        "print(\"Top Architecture Weights\")\n",
        "print(\"W1:\", network_top.W1)\n",
        "print(\"W2:\", network_top.W2)\n",
        "print(\"W_bypass:\", network_top.W_bypass)\n",
        "\n",
        "# Initialize and train the bottom architecture network\n",
        "print(\"\\nTraining Bottom Architecture Network\")\n",
        "network_bottom = BypassNetworkBottom(input_size=2, hidden_size=3, learning_rate=0.01)\n",
        "network_bottom.train(inputs, labels, epochs=1000)\n",
        "\n",
        "# Print final weights of the bottom architecture\n",
        "print(\"Bottom Architecture Weights\")\n",
        "print(\"W1:\", network_bottom.W1)\n",
        "print(\"W2:\", network_bottom.W2)\n",
        "print(\"W_bypass:\", network_bottom.W_bypass)"
      ],
      "metadata": {
        "id": "_DY3V77mI9rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The task is to design a multi-layer neural network for classification without using backpropagation. We need to find suitable weights manually. Since the activation functions are all step functions (unit step), we can design a simple network for binary classification.\n",
        "\n",
        "### Python Code\n",
        "\n",
        "Below is the Python code to manually design a neural network for the given classification task:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Define the neural network class\n",
        "class SimpleClassifier:\n",
        "    def __init__(self, W1, W2):\n",
        "        self.W1 = W1\n",
        "        self.W2 = W2\n",
        "\n",
        "    def forward(self, p):\n",
        "        # Layer 1\n",
        "        a1 = np.dot(self.W1, p)\n",
        "        h1 = step(a1)\n",
        "\n",
        "        # Layer 2\n",
        "        a2 = np.dot(self.W2, h1)\n",
        "        h2 = step(a2)\n",
        "\n",
        "        return h2\n",
        "\n",
        "# Define the weights manually based on the plot\n",
        "# These weights are chosen to separate the two classes as shown in the image\n",
        "W1 = np.array([[1, -1],\n",
        "               [1, 1]])\n",
        "W2 = np.array([[1, -1]])\n",
        "\n",
        "# Create the classifier\n",
        "classifier = SimpleClassifier(W1, W2)\n",
        "\n",
        "# Define the inputs (coordinates from the plot)\n",
        "inputs = np.array([[0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "                   [0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
        "\n",
        "# Labels based on visual inspection from the plot\n",
        "labels = np.array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(inputs[0], inputs[1], c=labels, cmap='bwr')\n",
        "plt.xlabel('p1')\n",
        "plt.ylabel('p2')\n",
        "plt.title('Data Points for Classification')\n",
        "plt.grid(True)\n",
        "\n",
        "# Test the classifier\n",
        "predictions = np.array([classifier.forward(p) for p in inputs.T]).flatten()\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual labels:\", labels)\n",
        "\n",
        "# Plot the decision boundary\n",
        "x = np.linspace(-1, 5, 100)\n",
        "y1 = -1 * x + 2  # Line corresponding to the first neuron in layer 1\n",
        "y2 = x           # Line corresponding to the second neuron in layer 1\n",
        "\n",
        "plt.plot(x, y1, 'k--', label='Neuron 1')\n",
        "plt.plot(x, y2, 'k--', label='Neuron 2')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Step Activation Function:**\n",
        "   - `step` function to implement the unit step function.\n",
        "\n",
        "2. **Neural Network Class:**\n",
        "   - `SimpleClassifier` class initializes with manually defined weights `W1` and `W2`.\n",
        "   - `forward` method computes the forward pass through the network using the step activation function.\n",
        "\n",
        "3. **Weights Definition:**\n",
        "   - `W1` and `W2` are defined manually to separate the classes based on the plot.\n",
        "   - `W1` has weights for two neurons in the first layer, and `W2` has weights for one neuron in the second layer.\n",
        "\n",
        "4. **Inputs and Labels:**\n",
        "   - `inputs` array contains the coordinates of the points from the plot.\n",
        "   - `labels` array contains the class labels determined visually from the plot.\n",
        "\n",
        "5. **Plotting:**\n",
        "   - The data points are plotted with different colors for different classes.\n",
        "   - Decision boundaries for the neurons in the first layer are plotted as dashed lines.\n",
        "\n",
        "6. **Testing the Classifier:**\n",
        "   - The classifier is tested on the inputs, and the predictions are printed.\n",
        "   - The decision boundary is plotted on the same plot for visual verification.\n",
        "\n",
        "This code manually defines a simple neural network and tests it on the given data points. The weights are chosen to separate the classes based on visual inspection of the plot."
      ],
      "metadata": {
        "id": "uehyEI8-HqTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task is to design a multi-layer neural network for classification without using backpropagation. We need to find suitable weights manually. Since the activation functions are all step functions (unit step), we can design a simple network for binary classification.\n",
        "\n",
        "### Python Code\n",
        "\n",
        "Below is the Python code to manually design a neural network for the given classification task:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# Define the neural network class\n",
        "class SimpleClassifier:\n",
        "    def __init__(self, W1, W2):\n",
        "        self.W1 = W1\n",
        "        self.W2 = W2\n",
        "\n",
        "    def forward(self, p):\n",
        "        # Layer 1\n",
        "        a1 = np.dot(self.W1, p)\n",
        "        h1 = step(a1)\n",
        "        \n",
        "        # Layer 2\n",
        "        a2 = np.dot(self.W2, h1)\n",
        "        h2 = step(a2)\n",
        "        \n",
        "        return h2\n",
        "\n",
        "# Define the weights manually based on the plot\n",
        "# These weights are chosen to separate the two classes as shown in the image\n",
        "W1 = np.array([[1, -1],\n",
        "               [1, 1]])\n",
        "W2 = np.array([[1, -1]])\n",
        "\n",
        "# Create the classifier\n",
        "classifier = SimpleClassifier(W1, W2)\n",
        "\n",
        "# Define the inputs (coordinates from the plot)\n",
        "inputs = np.array([[0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
        "                   [0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
        "\n",
        "# Labels based on visual inspection from the plot\n",
        "labels = np.array([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(inputs[0], inputs[1], c=labels, cmap='bwr')\n",
        "plt.xlabel('p1')\n",
        "plt.ylabel('p2')\n",
        "plt.title('Data Points for Classification')\n",
        "plt.grid(True)\n",
        "\n",
        "# Test the classifier\n",
        "predictions = np.array([classifier.forward(p) for p in inputs.T]).flatten()\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual labels:\", labels)\n",
        "\n",
        "# Plot the decision boundary\n",
        "x = np.linspace(-1, 5, 100)\n",
        "y1 = -1 * x + 2  # Line corresponding to the first neuron in layer 1\n",
        "y2 = x           # Line corresponding to the second neuron in layer 1\n",
        "\n",
        "plt.plot(x, y1, 'k--', label='Neuron 1')\n",
        "plt.plot(x, y2, 'k--', label='Neuron 2')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Step Activation Function:**\n",
        "   - `step` function to implement the unit step function.\n",
        "\n",
        "2. **Neural Network Class:**\n",
        "   - `SimpleClassifier` class initializes with manually defined weights `W1` and `W2`.\n",
        "   - `forward` method computes the forward pass through the network using the step activation function.\n",
        "\n",
        "3. **Weights Definition:**\n",
        "   - `W1` and `W2` are defined manually to separate the classes based on the plot.\n",
        "   - `W1` has weights for two neurons in the first layer, and `W2` has weights for one neuron in the second layer.\n",
        "\n",
        "4. **Inputs and Labels:**\n",
        "   - `inputs` array contains the coordinates of the points from the plot.\n",
        "   - `labels` array contains the class labels determined visually from the plot.\n",
        "\n",
        "5. **Plotting:**\n",
        "   - The data points are plotted with different colors for different classes.\n",
        "   - Decision boundaries for the neurons in the first layer are plotted as dashed lines.\n",
        "\n",
        "6. **Testing the Classifier:**\n",
        "   - The classifier is tested on the inputs, and the predictions are printed.\n",
        "   - The decision boundary is plotted on the same plot for visual verification.\n",
        "\n",
        "This code manually defines a simple neural network and tests it on the given data points. The weights are chosen to separate the classes based on visual inspection of the plot."
      ],
      "metadata": {
        "id": "KHH3UBw-KMvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I provided code for a single neural network with a bypass connection. Let's expand this to include both network architectures shown in the image.\n",
        "\n",
        "Below is the Python code for both bypassed networks:\n",
        "\n",
        "### Python Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "# Derivative of activation functions\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Mean Squared Error loss function\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Derivative of Mean Squared Error loss function\n",
        "def mse_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# Neural Network class for the top architecture\n",
        "class BypassNetworkTop:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size)\n",
        "        self.W2 = np.random.randn(1, hidden_size)\n",
        "        self.W_bypass = np.random.randn(1, input_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, p):\n",
        "        self.p = p\n",
        "        self.a1 = np.dot(self.W1, self.p)\n",
        "        self.h1 = relu(self.a1)\n",
        "        self.h2 = np.dot(self.W2, self.h1) + np.dot(self.W_bypass, self.p)\n",
        "        return linear(self.h2)\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        loss_grad = mse_derivative(y_true, y_pred)\n",
        "        dW2 = loss_grad * self.h1\n",
        "        dW_bypass = loss_grad * self.p\n",
        "        dW1 = np.dot((loss_grad * self.W2 * relu_derivative(self.a1)).reshape(-1, 1), self.p.reshape(1, -1))\n",
        "        \n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.W_bypass -= self.learning_rate * dW_bypass\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "\n",
        "    def train(self, inputs, labels, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for p, y_true in zip(inputs, labels):\n",
        "                y_pred = self.forward(p)\n",
        "                loss = mse_loss(y_true, y_pred)\n",
        "                total_loss += loss\n",
        "                self.backward(y_true, y_pred)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {total_loss / len(inputs)}')\n",
        "\n",
        "# Neural Network class for the bottom architecture\n",
        "class BypassNetworkBottom:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(hidden_size, input_size)\n",
        "        self.W2 = np.random.randn(1, hidden_size)\n",
        "        self.W_bypass = np.random.randn(1, input_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, p):\n",
        "        self.p = p\n",
        "        self.a1 = np.dot(self.W1, self.p)\n",
        "        self.h1 = relu(self.a1)\n",
        "        self.h2 = np.dot(self.W2, self.h1) + np.dot(self.W_bypass, self.p)\n",
        "        return linear(self.h2)\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        loss_grad = mse_derivative(y_true, y_pred)\n",
        "        dW2 = loss_grad * self.h1\n",
        "        dW_bypass = loss_grad * self.p\n",
        "        dW1 = np.dot((loss_grad * self.W2 * relu_derivative(self.a1)).reshape(-1, 1), self.p.reshape(1, -1))\n",
        "        \n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.W_bypass -= self.learning_rate * dW_bypass\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "\n",
        "    def train(self, inputs, labels, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for p, y_true in zip(inputs, labels):\n",
        "                y_pred = self.forward(p)\n",
        "                loss = mse_loss(y_true, y_pred)\n",
        "                total_loss += loss\n",
        "                self.backward(y_true, y_pred)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {total_loss / len(inputs)}')\n",
        "\n",
        "# Training data\n",
        "inputs = [np.array([1, 2]), np.array([2, -3]), np.array([0, 2]), np.array([-1, -4])]\n",
        "labels = [2.2, 7.5, 9.2, 8.1]\n",
        "\n",
        "# Initialize and train the top architecture network\n",
        "print(\"Training Top Architecture Network\")\n",
        "network_top = BypassNetworkTop(input_size=2, hidden_size=3, learning_rate=0.01)\n",
        "network_top.train(inputs, labels, epochs=1000)\n",
        "\n",
        "# Print final weights of the top architecture\n",
        "print(\"Top Architecture Weights\")\n",
        "print(\"W1:\", network_top.W1)\n",
        "print(\"W2:\", network_top.W2)\n",
        "print(\"W_bypass:\", network_top.W_bypass)\n",
        "\n",
        "# Initialize and train the bottom architecture network\n",
        "print(\"\\nTraining Bottom Architecture Network\")\n",
        "network_bottom = BypassNetworkBottom(input_size=2, hidden_size=3, learning_rate=0.01)\n",
        "network_bottom.train(inputs, labels, epochs=1000)\n",
        "\n",
        "# Print final weights of the bottom architecture\n",
        "print(\"Bottom Architecture Weights\")\n",
        "print(\"W1:\", network_bottom.W1)\n",
        "print(\"W2:\", network_bottom.W2)\n",
        "print(\"W_bypass:\", network_bottom.W_bypass)\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "1. **Activation Functions and Their Derivatives:**\n",
        "   - `relu` and `relu_derivative` are used for the hidden layer.\n",
        "   - `linear` is used for the output layer.\n",
        "\n",
        "2. **Loss Functions:**\n",
        "   - `mse_loss` calculates the Mean Squared Error.\n",
        "   - `mse_derivative` computes the gradient of the MSE loss.\n",
        "\n",
        "3. **BypassNetworkTop and BypassNetworkBottom Classes:**\n",
        "   - Both classes initialize the network with random weights.\n",
        "   - The `forward` method computes the network's output.\n",
        "   - The `backward` method computes the gradients and updates the weights using gradient descent.\n",
        "   - The `train` method trains the network over a specified number of epochs.\n",
        "\n",
        "4. **Training Data:**\n",
        "   - Inputs and corresponding labels are provided.\n",
        "   - Both networks are trained for 1000 epochs with a learning rate of 0.01.\n",
        "\n",
        "5. **Final Weights:**\n",
        "   - The final weights of both networks are printed after training.\n",
        "\n",
        "This code defines two separate classes for the two different bypassed network architectures shown in the image and trains them using the same dataset."
      ],
      "metadata": {
        "id": "KzKl3oIOK-7i"
      }
    }
  ]
}