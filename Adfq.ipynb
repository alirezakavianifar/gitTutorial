{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPreion/5RqsjdLbAi0T1Xh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakavianifar/gitTutorial/blob/developer/Adfq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEuY-sjIAEsP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The article introduces a novel Bayesian approach to Q-learning, named Assumed Density Filtering Q-learning (ADFQ). The key contributions and novelties of the article are:\n",
        "\n",
        "1. **Bayesian Off-Policy TD Method**: ADFQ is an off-policy temporal difference (TD) method that leverages Bayesian inference to update beliefs about state-action values, \\( Q \\), using Assumed Density Filtering (ADF). This is significant because Bayesian methods explicitly quantify uncertainty, which can improve exploration strategies and provide natural regularization for value updates【6:0†source】【6:3†source】.\n",
        "\n",
        "2. **Non-Greedy Updates with Uncertainty Regularization**: Unlike conventional Q-learning, ADFQ incorporates the information from all available actions in the subsequent state into the Q-value update. It uses uncertainty measures not only for exploration but also as regularization in the value update, which helps in managing overoptimism and instability issues often seen in standard Q-learning【6:2†source】【6:4†source】.\n",
        "\n",
        "3. **Computational Efficiency**: One of the significant drawbacks of Bayesian RL approaches is their computational complexity. ADFQ addresses this by providing an efficient closed-form solution for the value update. It approximates the posterior parameters analytically, reducing the computational burden and making the approach scalable to larger problems【6:0†source】【6:4†source】.\n",
        "\n",
        "4. **Empirical Performance**: The article demonstrates that ADFQ outperforms traditional Q-learning and its extensions like Deep Q-Networks (DQN) in various Atari 2600 games. The performance improvements are particularly notable in environments with high stochasticity or large action spaces, where ADFQ's non-greedy updates with uncertainty measures significantly enhance learning stability and efficiency【6:2†source】【6:5†source】.\n",
        "\n",
        "In summary, the novelty of the article lies in developing a Bayesian Q-learning algorithm that effectively balances exploration and exploitation through uncertainty-aware updates, providing computational efficiency and superior performance in complex, high-stakes environments."
      ],
      "metadata": {
        "id": "C-z_sFohAKfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The article provides an implementation of the Assumed Density Filtering Q-learning (ADFQ) algorithm. Below is the ADFQ algorithm as described in the article:\n",
        "\n",
        "### ADFQ Algorithm\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class ADFQ:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=0.1, sigma_w=0.1):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.sigma_w = sigma_w\n",
        "        self.mu = np.zeros((state_dim, action_dim))  # Mean of Q-values\n",
        "        self.sigma2 = np.ones((state_dim, action_dim))  # Variance of Q-values\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        # Thompson Sampling for action selection\n",
        "        sampled_Q = np.random.normal(self.mu[state], np.sqrt(self.sigma2[state]))\n",
        "        return np.argmax(sampled_Q)\n",
        "    \n",
        "    def update(self, state, action, reward, next_state):\n",
        "        # Compute the value and variance for all actions in the next state\n",
        "        V_next_mean = np.max(self.mu[next_state])\n",
        "        V_next_var = np.min(self.sigma2[next_state])\n",
        "        \n",
        "        # Likelihood parameters for the Bellman update\n",
        "        target_mean = reward + self.gamma * V_next_mean\n",
        "        target_var = self.gamma**2 * V_next_var + self.sigma_w**2\n",
        "        \n",
        "        # Update mean and variance of Q-values using Bayesian update rules\n",
        "        prior_mean = self.mu[state, action]\n",
        "        prior_var = self.sigma2[state, action]\n",
        "        \n",
        "        post_mean = (target_mean / target_var + prior_mean / prior_var) / (1 / target_var + 1 / prior_var)\n",
        "        post_var = 1 / (1 / target_var + 1 / prior_var)\n",
        "        \n",
        "        self.mu[state, action] = self.alpha * post_mean + (1 - self.alpha) * prior_mean\n",
        "        self.sigma2[state, action] = self.alpha * post_var + (1 - self.alpha) * prior_var\n",
        "\n",
        "# Example usage\n",
        "state_dim = 10  # Example state space dimension\n",
        "action_dim = 4  # Example action space dimension\n",
        "\n",
        "adfq = ADFQ(state_dim, action_dim)\n",
        "\n",
        "# Simulate an update step\n",
        "state = 0\n",
        "action = 1\n",
        "reward = 1\n",
        "next_state = 2\n",
        "\n",
        "adfq.update(state, action, reward, next_state)\n",
        "action = adfq.select_action(state)\n",
        "print(f\"Selected action: {action}\")\n",
        "```\n",
        "\n",
        "This code defines a simplified version of the ADFQ algorithm based on the description in the article. The algorithm initializes the Q-value means and variances, uses Thompson Sampling for action selection, and performs Bayesian updates for the Q-values based on the observed rewards and transitions【6:0†source】【6:1†source】【6:4†source】. For the complete implementation and further details, refer to the article and the provided GitHub repository: [ADFQ on GitHub](https://github.com/coco66/ADFQ)."
      ],
      "metadata": {
        "id": "5iXRV0lWAMic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The uncertainty-aware update in the Assumed Density Filtering Q-learning (ADFQ) algorithm is a key feature that differentiates it from traditional Q-learning methods. Here's an in-depth explanation of how this update works and why it is beneficial:\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "1. **Uncertainty Quantification**:\n",
        "   - ADFQ maintains both a mean (\\(\\mu\\)) and a variance (\\(\\sigma^2\\)) for each state-action pair, which represent the expected value and the uncertainty (variance) of the Q-values respectively. This allows the algorithm to keep track of the uncertainty in its estimates.\n",
        "\n",
        "2. **Bayesian Update**:\n",
        "   - When an agent takes an action and receives a reward, ADFQ updates its beliefs about the Q-values using Bayesian inference. This involves updating both the mean and variance of the Q-values.\n",
        "\n",
        "### Update Mechanism\n",
        "\n",
        "1. **Value and Variance of Next State**:\n",
        "   - For the next state \\( s' \\), ADFQ considers the Q-values for all possible actions \\( a' \\):\n",
        "     \\[\n",
        "     V_{next}^{mean} = \\max_{a'} \\mu(s', a')\n",
        "     \\]\n",
        "     \\[\n",
        "     V_{next}^{var} = \\min_{a'} \\sigma^2(s', a')\n",
        "     \\]\n",
        "   - Using the maximum expected value and the minimum variance of the next state ensures a conservative estimate of future rewards.\n",
        "\n",
        "2. **Target Value Distribution**:\n",
        "   - The target mean and variance for the Bellman update are computed as:\n",
        "     \\[\n",
        "     \\text{target\\_mean} = r + \\gamma V_{next}^{mean}\n",
        "     \\]\n",
        "     \\[\n",
        "     \\text{target\\_var} = \\gamma^2 V_{next}^{var} + \\sigma_w^2\n",
        "     \\]\n",
        "   - Here, \\( r \\) is the reward, \\( \\gamma \\) is the discount factor, and \\( \\sigma_w \\) is the assumed noise variance.\n",
        "\n",
        "3. **Posterior Update**:\n",
        "   - The posterior mean and variance are updated using Bayesian rules:\n",
        "     \\[\n",
        "     \\text{post\\_mean} = \\frac{\\text{target\\_mean} / \\text{target\\_var} + \\mu(s, a) / \\sigma^2(s, a)}{1 / \\text{target\\_var} + 1 / \\sigma^2(s, a)}\n",
        "     \\]\n",
        "     \\[\n",
        "     \\text{post\\_var} = \\frac{1}{1 / \\text{target\\_var} + 1 / \\sigma^2(s, a)}\n",
        "     \\]\n",
        "   - This combines the prior belief (\\(\\mu(s, a)\\) and \\(\\sigma^2(s, a)\\)) with the new information from the observed transition.\n",
        "\n",
        "4. **Update Step with Regularization**:\n",
        "   - Finally, the Q-value mean and variance are updated with a learning rate \\(\\alpha\\):\n",
        "     \\[\n",
        "     \\mu(s, a) \\leftarrow \\alpha \\cdot \\text{post\\_mean} + (1 - \\alpha) \\cdot \\mu(s, a)\n",
        "     \\]\n",
        "     \\[\n",
        "     \\sigma^2(s, a) \\leftarrow \\alpha \\cdot \\text{post\\_var} + (1 - \\alpha) \\cdot \\sigma^2(s, a)\n",
        "     \\]\n",
        "   - This ensures a smooth update, incorporating both the new information and retaining some of the previous estimates.\n",
        "\n",
        "### Benefits of Uncertainty-Aware Updates\n",
        "\n",
        "1. **Improved Exploration**:\n",
        "   - By maintaining an estimate of uncertainty, ADFQ can use strategies like Thompson Sampling, which balances exploration and exploitation more effectively. Actions with high uncertainty are more likely to be explored, which can lead to better policy learning.\n",
        "\n",
        "2. **Stability and Regularization**:\n",
        "   - The variance term acts as a regularizer in the update process, which can help in avoiding overestimation or underestimation of Q-values. This is particularly beneficial in environments with high stochasticity or noise.\n",
        "\n",
        "3. **Adaptability to Non-Stationarity**:\n",
        "   - The Bayesian updates allow the algorithm to adapt more quickly to changes in the environment, as it continuously refines its beliefs about the state-action values based on observed data.\n",
        "\n",
        "In summary, the uncertainty-aware updates in ADFQ enhance the learning process by providing a principled way to incorporate uncertainty into Q-value estimates, leading to better exploration, more stable updates, and improved adaptability to dynamic environments【6:2†source】【6:4†source】【6:5†source】."
      ],
      "metadata": {
        "id": "QmKFECYvATCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's clarify the uncertainty-aware update in the ADFQ algorithm with a step-by-step example.\n",
        "\n",
        "### Example Scenario\n",
        "\n",
        "- **State Space (S)**: {0, 1, 2}\n",
        "- **Action Space (A)**: {0, 1}\n",
        "- **Initial Q-values (means)**:\n",
        "  \\[\n",
        "  \\mu(0, 0) = 1.0, \\quad \\mu(0, 1) = 2.0\n",
        "  \\]\n",
        "  \\[\n",
        "  \\mu(1, 0) = 3.0, \\quad \\mu(1, 1) = 0.5\n",
        "  \\]\n",
        "  \\[\n",
        "  \\mu(2, 0) = 0.0, \\quad \\mu(2, 1) = 1.5\n",
        "  \\]\n",
        "- **Initial Q-value variances**:\n",
        "  \\[\n",
        "  \\sigma^2(0, 0) = 0.5, \\quad \\sigma^2(0, 1) = 0.2\n",
        "  \\]\n",
        "  \\[\n",
        "  \\sigma^2(1, 0) = 0.3, \\quad \\sigma^2(1, 1) = 0.1\n",
        "  \\]\n",
        "  \\[\n",
        "  \\sigma^2(2, 0) = 0.6, \\quad \\sigma^2(2, 1) = 0.4\n",
        "  \\]\n",
        "\n",
        "### Parameters\n",
        "\n",
        "- Discount factor (\\(\\gamma\\)): 0.9\n",
        "- Learning rate (\\(\\alpha\\)): 0.1\n",
        "- Noise variance (\\(\\sigma_w^2\\)): 0.1\n",
        "\n",
        "### Step-by-Step Update\n",
        "\n",
        "**1. Initial State and Action Selection**\n",
        "\n",
        "- The agent is in state \\(s = 0\\).\n",
        "- It selects action \\(a = 1\\) using Thompson Sampling or another policy.\n",
        "\n",
        "**2. Observing Reward and Next State**\n",
        "\n",
        "- The agent receives a reward \\(r = 5\\).\n",
        "- The agent transitions to the next state \\(s' = 1\\).\n",
        "\n",
        "**3. Compute Next State Values**\n",
        "\n",
        "- Maximum mean Q-value in state \\(s' = 1\\):\n",
        "  \\[\n",
        "  V_{next}^{mean} = \\max\\left(\\mu(1, 0), \\mu(1, 1)\\right) = \\max(3.0, 0.5) = 3.0\n",
        "  \\]\n",
        "- Minimum variance Q-value in state \\(s' = 1\\):\n",
        "  \\[\n",
        "  V_{next}^{var} = \\min\\left(\\sigma^2(1, 0), \\sigma^2(1, 1)\\right) = \\min(0.3, 0.1) = 0.1\n",
        "  \\]\n",
        "\n",
        "**4. Compute Target Value Distribution**\n",
        "\n",
        "- Target mean:\n",
        "  \\[\n",
        "  \\text{target\\_mean} = r + \\gamma V_{next}^{mean} = 5 + 0.9 \\times 3.0 = 7.7\n",
        "  \\]\n",
        "- Target variance:\n",
        "  \\[\n",
        "  \\text{target\\_var} = \\gamma^2 V_{next}^{var} + \\sigma_w^2 = 0.9^2 \\times 0.1 + 0.1 = 0.081 + 0.1 = 0.181\n",
        "  \\]\n",
        "\n",
        "**5. Bayesian Update**\n",
        "\n",
        "- Prior mean and variance for \\(Q(0, 1)\\):\n",
        "  \\[\n",
        "  \\text{prior\\_mean} = \\mu(0, 1) = 2.0\n",
        "  \\]\n",
        "  \\[\n",
        "  \\text{prior\\_var} = \\sigma^2(0, 1) = 0.2\n",
        "  \\]\n",
        "- Posterior mean:\n",
        "  \\[\n",
        "  \\text{post\\_mean} = \\frac{\\text{target\\_mean} / \\text{target\\_var} + \\text{prior\\_mean} / \\text{prior\\_var}}{1 / \\text{target\\_var} + 1 / \\text{prior\\_var}}\n",
        "  \\]\n",
        "  \\[\n",
        "  \\text{post\\_mean} = \\frac{7.7 / 0.181 + 2.0 / 0.2}{1 / 0.181 + 1 / 0.2} = \\frac{42.54 + 10}{5.52 + 5} = \\frac{52.54}{10.52} \\approx 4.99\n",
        "  \\]\n",
        "- Posterior variance:\n",
        "  \\[\n",
        "  \\text{post\\_var} = \\frac{1}{1 / \\text{target\\_var} + 1 / \\text{prior\\_var}} = \\frac{1}{5.52 + 5} \\approx 0.095\n",
        "  \\]\n",
        "\n",
        "**6. Update Q-Values with Regularization**\n",
        "\n",
        "- Updated mean for \\(Q(0, 1)\\):\n",
        "  \\[\n",
        "  \\mu(0, 1) \\leftarrow \\alpha \\cdot \\text{post\\_mean} + (1 - \\alpha) \\cdot \\text{prior\\_mean}\n",
        "  \\]\n",
        "  \\[\n",
        "  \\mu(0, 1) \\leftarrow 0.1 \\cdot 4.99 + 0.9 \\cdot 2.0 = 0.499 + 1.8 = 2.299\n",
        "  \\]\n",
        "- Updated variance for \\(Q(0, 1)\\):\n",
        "  \\[\n",
        "  \\sigma^2(0, 1) \\leftarrow \\alpha \\cdot \\text{post\\_var} + (1 - \\alpha) \\cdot \\text{prior\\_var}\n",
        "  \\]\n",
        "  \\[\n",
        "  \\sigma^2(0, 1) \\leftarrow 0.1 \\cdot 0.095 + 0.9 \\cdot 0.2 = 0.0095 + 0.18 = 0.1895\n",
        "  \\]\n",
        "\n",
        "### Summary of Update\n",
        "\n",
        "- The Q-value for state 0, action 1 is updated to a new mean of 2.299 and a new variance of 0.1895.\n",
        "- This update incorporates both the immediate reward and the expected future rewards, while also adjusting for the uncertainty in these estimates.\n",
        "\n",
        "By including the variance in the update, ADFQ can better handle uncertainty in the environment, leading to more stable and reliable learning. This helps prevent over-optimistic estimates and ensures that the algorithm remains robust even in noisy or unpredictable environments."
      ],
      "metadata": {
        "id": "JckuelZ3CBi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the Deep Assumed Density Filtering Q-learning (Deep ADFQ) algorithm, we will extend the basic ADFQ algorithm to handle high-dimensional state spaces using a neural network to approximate the Q-values. The neural network will output both the mean and variance of the Q-values for each action given a state. Here’s a step-by-step implementation in Python using PyTorch.\n",
        "\n",
        "### Step-by-Step Implementation\n",
        "\n",
        "1. **Install PyTorch**: Ensure you have PyTorch installed. You can install it using pip if you haven't already:\n",
        "   ```sh\n",
        "   pip install torch\n",
        "   ```\n",
        "\n",
        "2. **Define the Neural Network**: The network will output both the mean and variance for each action.\n",
        "\n",
        "3. **Implement the Deep ADFQ Algorithm**:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network architecture\n",
        "class DeepADFQNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DeepADFQNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc_mean = nn.Linear(128, action_dim)\n",
        "        self.fc_var = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        var = torch.softplus(self.fc_var(x))  # Ensure variance is positive\n",
        "        return mean, var\n",
        "\n",
        "# Deep ADFQ agent\n",
        "class DeepADFQAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=0.1, sigma_w=0.1, lr=0.001):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.sigma_w = sigma_w\n",
        "        \n",
        "        self.model = DeepADFQNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        mean, _ = self.model(state_tensor)\n",
        "        mean = mean.detach().numpy()[0]\n",
        "        return np.argmax(mean)\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        \n",
        "        mean, var = self.model(state_tensor)\n",
        "        next_mean, next_var = self.model(next_state_tensor)\n",
        "\n",
        "        mean = mean[0, action]\n",
        "        var = var[0, action]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            V_next_mean = torch.max(next_mean)\n",
        "            V_next_var = torch.min(next_var)\n",
        "            target_mean = reward + self.gamma * V_next_mean * (1 - int(done))\n",
        "            target_var = (self.gamma ** 2) * V_next_var + self.sigma_w ** 2\n",
        "\n",
        "            prior_mean = mean\n",
        "            prior_var = var\n",
        "\n",
        "            post_mean = (target_mean / target_var + prior_mean / prior_var) / (1 / target_var + 1 / prior_var)\n",
        "            post_var = 1 / (1 / target_var + 1 / prior_var)\n",
        "\n",
        "        loss_mean = self.criterion(mean, post_mean)\n",
        "        loss_var = self.criterion(var, post_var)\n",
        "        loss = loss_mean + loss_var\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# Example usage\n",
        "state_dim = 4  # Example state space dimension\n",
        "action_dim = 2  # Example action space dimension\n",
        "agent = DeepADFQAgent(state_dim, action_dim)\n",
        "\n",
        "# Simulate an update step\n",
        "state = np.random.rand(state_dim)\n",
        "action = 1\n",
        "reward = 1\n",
        "next_state = np.random.rand(state_dim)\n",
        "done = False\n",
        "\n",
        "agent.update(state, action, reward, next_state, done)\n",
        "action = agent.select_action(state)\n",
        "print(f\"Selected action: {action}\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **DeepADFQNetwork**: This neural network takes the state as input and outputs two vectors: one for the mean Q-values and another for the variances of the Q-values for each action. The variance output is passed through a `softplus` function to ensure it's always positive.\n",
        "\n",
        "2. **DeepADFQAgent**:\n",
        "   - **Initialization**: Initializes the neural network, optimizer, and loss function.\n",
        "   - **select_action**: Uses the network to get the mean Q-values and selects the action with the highest mean Q-value.\n",
        "   - **update**: Performs a Bayesian update of the Q-value mean and variance using the network’s predictions and the observed reward and transition.\n",
        "\n",
        "3. **Example Usage**: Demonstrates how to create an agent, simulate a transition, update the Q-values, and select an action.\n",
        "\n",
        "This implementation captures the essence of Deep ADFQ, leveraging deep learning to approximate Q-values while incorporating uncertainty through variance estimation and Bayesian updates."
      ],
      "metadata": {
        "id": "PidYXTaADQpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, here's how you can implement the Deep Assumed Density Filtering Q-learning (Deep ADFQ) algorithm using TensorFlow.\n",
        "\n",
        "### Step-by-Step Implementation in TensorFlow\n",
        "\n",
        "1. **Install TensorFlow**: Ensure you have TensorFlow installed. You can install it using pip if you haven't already:\n",
        "   ```sh\n",
        "   pip install tensorflow\n",
        "   ```\n",
        "\n",
        "2. **Define the Neural Network**: The network will output both the mean and variance for each action given a state.\n",
        "\n",
        "3. **Implement the Deep ADFQ Algorithm**:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network architecture\n",
        "class DeepADFQNetwork(tf.keras.Model):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DeepADFQNetwork, self).__init__()\n",
        "        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.fc_mean = tf.keras.layers.Dense(action_dim)\n",
        "        self.fc_var = tf.keras.layers.Dense(action_dim, activation='softplus')  # Ensure variance is positive\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        mean = self.fc_mean(x)\n",
        "        var = self.fc_var(x)\n",
        "        return mean, var\n",
        "\n",
        "# Deep ADFQ agent\n",
        "class DeepADFQAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=0.1, sigma_w=0.1, lr=0.001):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.sigma_w = sigma_w\n",
        "        \n",
        "        self.model = DeepADFQNetwork(state_dim, action_dim)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        mean, _ = self.model(state_tensor)\n",
        "        mean = mean.numpy()[0]\n",
        "        return np.argmax(mean)\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        next_state_tensor = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            mean, var = self.model(state_tensor, training=True)\n",
        "            next_mean, next_var = self.model(next_state_tensor, training=False)\n",
        "\n",
        "            mean = mean[0, action]\n",
        "            var = var[0, action]\n",
        "\n",
        "            V_next_mean = tf.reduce_max(next_mean)\n",
        "            V_next_var = tf.reduce_min(next_var)\n",
        "            target_mean = reward + self.gamma * V_next_mean * (1 - int(done))\n",
        "            target_var = (self.gamma ** 2) * V_next_var + self.sigma_w ** 2\n",
        "\n",
        "            prior_mean = mean\n",
        "            prior_var = var\n",
        "\n",
        "            post_mean = (target_mean / target_var + prior_mean / prior_var) / (1 / target_var + 1 / prior_var)\n",
        "            post_var = 1 / (1 / target_var + 1 / prior_var)\n",
        "\n",
        "            loss_mean = self.loss_fn(mean, post_mean)\n",
        "            loss_var = self.loss_fn(var, post_var)\n",
        "            loss = loss_mean + loss_var\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "# Example usage\n",
        "state_dim = 4  # Example state space dimension\n",
        "action_dim = 2  # Example action space dimension\n",
        "agent = DeepADFQAgent(state_dim, action_dim)\n",
        "\n",
        "# Simulate an update step\n",
        "state = np.random.rand(state_dim)\n",
        "action = 1\n",
        "reward = 1\n",
        "next_state = np.random.rand(state_dim)\n",
        "done = False\n",
        "\n",
        "agent.update(state, action, reward, next_state, done)\n",
        "action = agent.select_action(state)\n",
        "print(f\"Selected action: {action}\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **DeepADFQNetwork**: This neural network takes the state as input and outputs two vectors: one for the mean Q-values and another for the variances of the Q-values for each action. The variance output is passed through a `softplus` activation to ensure it's always positive.\n",
        "\n",
        "2. **DeepADFQAgent**:\n",
        "   - **Initialization**: Initializes the neural network, optimizer, and loss function.\n",
        "   - **select_action**: Uses the network to get the mean Q-values and selects the action with the highest mean Q-value.\n",
        "   - **update**: Performs a Bayesian update of the Q-value mean and variance using the network’s predictions and the observed reward and transition. The update uses TensorFlow's `GradientTape` to compute and apply gradients.\n",
        "\n",
        "3. **Example Usage**: Demonstrates how to create an agent, simulate a transition, update the Q-values, and select an action.\n",
        "\n",
        "This implementation captures the core aspects of Deep ADFQ, leveraging deep learning to approximate Q-values while incorporating uncertainty through variance estimation and Bayesian updates."
      ],
      "metadata": {
        "id": "Z6pIODeYDsaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The article evaluates the Deep Assumed Density Filtering Q-learning (Deep ADFQ) algorithm through a series of case studies focused on different environments and scenarios. Here are the key case studies discussed:\n",
        "\n",
        "1. **Tabular Domains**:\n",
        "   - **Cliff Walking**: A classic reinforcement learning benchmark where the agent must navigate a grid world to reach a goal while avoiding a \"cliff\" area that incurs a high negative reward.\n",
        "   - **Frozen Lake**: Another grid world scenario where the agent must navigate a slippery frozen lake to reach a goal, dealing with both the stochasticity of the environment and potential pitfalls.\n",
        "\n",
        "2. **Function Approximation Domains**:\n",
        "   - **Mountain Car**: A continuous control problem where the agent must drive an underpowered car up a hill. The agent receives a reward when it successfully reaches the top of the hill.\n",
        "   - **Cart Pole**: A balance control problem where the agent must balance a pole on a moving cart. The agent receives a reward for each time step the pole remains upright.\n",
        "\n",
        "3. **Deep Learning Domains**:\n",
        "   - **Atari Games**: The article specifically mentions using a subset of Atari games from the Arcade Learning Environment to test the performance of Deep ADFQ in more complex, high-dimensional state spaces typical of deep reinforcement learning benchmarks.\n",
        "\n",
        "These case studies cover a wide range of difficulties and complexities, from simple tabular problems to more challenging continuous control and high-dimensional tasks. This variety allows the authors to demonstrate the versatility and robustness of the Deep ADFQ algorithm across different types of reinforcement learning environments ."
      ],
      "metadata": {
        "id": "us17sjNB3cNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Deep Assumed Density Filtering Q-learning (Deep ADFQ) algorithm incorporates both the mean and the variance of the Q-values in its decision-making process. However, the selection of actions in ADFQ is typically done by considering the trade-off between exploration and exploitation, similar to other Q-learning approaches. Here’s a more detailed explanation:\n",
        "\n",
        "### Action Selection in ADFQ\n",
        "\n",
        "1. **Thompson Sampling**: ADFQ often uses a form of Thompson Sampling to select actions. In this approach, the algorithm samples from the Q-value distribution (defined by the mean and variance) for each action and then selects the action with the highest sampled value. This naturally balances exploration and exploitation by considering the uncertainty (variance) in the Q-values.\n",
        "   - **Sampling Q-values**: For each action \\(a\\) in state \\(s\\), sample a Q-value from the normal distribution \\(N(\\mu(s, a), \\sigma^2(s, a))\\).\n",
        "   - **Action Selection**: Select the action with the highest sampled Q-value.\n",
        "\n",
        "2. **Greedy with Uncertainty Consideration**: Alternatively, the algorithm can directly incorporate both the mean and the variance in a more deterministic way. One approach is to select the action that maximizes a utility function that considers both the mean and the uncertainty (variance) of the Q-values. This utility function can be designed in various ways, such as:\n",
        "   - **Upper Confidence Bound (UCB)**: \\(Q(s, a) = \\mu(s, a) + \\beta \\cdot \\sigma(s, a)\\), where \\(\\beta\\) is a parameter that controls the trade-off between exploration (high variance) and exploitation (high mean).\n",
        "   - **Greedy with Minimum Variance**: Select the action with the highest mean, but in the case of ties, prefer the action with the smallest variance. This reduces the risk associated with actions having high uncertainty.\n",
        "\n",
        "### Example Implementation\n",
        "\n",
        "Here’s how you might implement the Thompson Sampling action selection in the context of the Deep ADFQ algorithm using TensorFlow:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class DeepADFQAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=0.1, sigma_w=0.1, lr=0.001):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.sigma_w = sigma_w\n",
        "        \n",
        "        self.model = DeepADFQNetwork(state_dim, action_dim)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        mean, var = self.model(state_tensor)\n",
        "        mean = mean.numpy()[0]\n",
        "        var = var.numpy()[0]\n",
        "        \n",
        "        sampled_q_values = np.random.normal(mean, np.sqrt(var))\n",
        "        return np.argmax(sampled_q_values)\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_tensor = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        next_state_tensor = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            mean, var = self.model(state_tensor, training=True)\n",
        "            next_mean, next_var = self.model(next_state_tensor, training=False)\n",
        "\n",
        "            mean = mean[0, action]\n",
        "            var = var[0, action]\n",
        "\n",
        "            V_next_mean = tf.reduce_max(next_mean)\n",
        "            V_next_var = tf.reduce_min(next_var)\n",
        "            target_mean = reward + self.gamma * V_next_mean * (1 - int(done))\n",
        "            target_var = (self.gamma ** 2) * V_next_var + self.sigma_w ** 2\n",
        "\n",
        "            prior_mean = mean\n",
        "            prior_var = var\n",
        "\n",
        "            post_mean = (target_mean / target_var + prior_mean / prior_var) / (1 / target_var + 1 / prior_var)\n",
        "            post_var = 1 / (1 / target_var + 1 / prior_var)\n",
        "\n",
        "            loss_mean = self.loss_fn(mean, post_mean)\n",
        "            loss_var = self.loss_fn(var, post_var)\n",
        "            loss = loss_mean + loss_var\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "# Example usage\n",
        "state_dim = 4  # Example state space dimension\n",
        "action_dim = 2  # Example action space dimension\n",
        "agent = DeepADFQAgent(state_dim, action_dim)\n",
        "\n",
        "# Simulate an update step\n",
        "state = np.random.rand(state_dim)\n",
        "action = 1\n",
        "reward = 1\n",
        "next_state = np.random.rand(state_dim)\n",
        "done = False\n",
        "\n",
        "agent.update(state, action, reward, next_state, done)\n",
        "action = agent.select_action(state)\n",
        "print(f\"Selected action: {action}\")\n",
        "```\n",
        "\n",
        "### Summary\n",
        "\n",
        "In summary, ADFQ can select actions using methods like Thompson Sampling, which naturally balances exploration and exploitation by sampling from the Q-value distributions. Alternatively, deterministic strategies like UCB can be used, where the action selection explicitly considers both the mean and variance of the Q-values to manage uncertainty. This allows ADFQ to handle environments with varying levels of stochasticity and ensure more robust learning outcomes."
      ],
      "metadata": {
        "id": "lhG43Pb26hhW"
      }
    }
  ]
}